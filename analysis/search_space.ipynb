{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34881024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [2, 4, 14, 2]\n",
    "# the channel number of each layer\n",
    "layer = np.array([64, 128, 256, 512])\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "maxpool = [True,False]\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "def search_space(stage_layer, layer):\n",
    "    sum = 1\n",
    "    for i in stage_layer_max:\n",
    "        sum *= i\n",
    "    for j in layer:\n",
    "        sum *= len(ratio)\n",
    "    return sum*16*16\n",
    "print(search_space(stage_layer_max,layer)/1e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choices \n",
    "choices(range(1,3))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices(ratio)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choices \n",
    "\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [2, 4, 14, 2]\n",
    "# the channel number of each layer\n",
    "layer_num_max = [64, 128, 256, 512]\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "maxpool = [True,False]\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "datasets = [\"cifar10\",\"cifar100\"]\n",
    "\n",
    "\n",
    "# operation \n",
    "# dataset = choices(datasets)[0]\n",
    "# class_num = 10 if dataset == \"cifar10\" else 100\n",
    "import yaml\n",
    "import os \n",
    "import collections\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    \"\"\"dict保存为yaml\"\"\"\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    " \n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "savedirs = []\n",
    "md_fmts = []\n",
    "for n in range(12,100):\n",
    "    config = {}\n",
    "    # 随机生成模型\n",
    "    stage_layer = [choices(range(1,stage_layer_max[0]+1))[0],choices(range(1,stage_layer_max[1]+1))[0],choices(range(1,stage_layer_max[2]+1))[0],choices(range(1,stage_layer_max[3]+1))[0]]\n",
    "    # stage_layer\n",
    "    stage_ratio = [choices(ratio)[0],choices(ratio)[0],choices(ratio)[0],choices(ratio)[0]]\n",
    "    # stage_ratio\n",
    "    with_maxpool = [choices(maxpool)[0],choices(maxpool)[0],choices(maxpool)[0],choices(maxpool)[0]]\n",
    "    # with_maxpool\n",
    "    op_type = [choices(op)[0],choices(op)[0],choices(op)[0],choices(op)[0],choices(op)[0]]\n",
    "    dir = str(n)+'-stage-' + str(stage_layer[0]) + '_' + str(stage_layer[1]) + '_' + str(stage_layer[2]) + '_' + str(stage_layer[3]) + '-ratio-' \\\n",
    "        + str(stage_ratio[0]) + '_' + str(stage_ratio[1]) + '_' + str(stage_ratio[2]) + '_' + str(stage_ratio[3]) + \\\n",
    "            '-op-' + str(op_type[0]) + '_' + str(op_type[1]) + '_' + str(op_type[2]) + '_' + str(op_type[3]) + '_' + str(op_type[4]) + '-max-' + \\\n",
    "                str(with_maxpool[0]) + '_' + str(with_maxpool[1]) + '_' + str(with_maxpool[2]) + '_' + str(with_maxpool[3])\n",
    "    md_fmt = \"|\"+ str(n)+'|' + str(stage_layer[0]) + '-' + str(stage_layer[1]) + '-' + str(stage_layer[2]) + '-' + str(stage_layer[3]) + '|' \\\n",
    "        + str(stage_ratio[0]) + '-' + str(stage_ratio[1]) + '-' + str(stage_ratio[2]) + '-' + str(stage_ratio[3]) + \\\n",
    "            '|' + str(op_type[0]) + '-' + str(op_type[1]) + '-' + str(op_type[2]) + '-' + str(op_type[3]) + '-' + str(op_type[4]) + '|' + \\\n",
    "                str(with_maxpool[0]) + '-' + str(with_maxpool[1]) + '-' + str(with_maxpool[2]) + '-' + str(with_maxpool[3]) + \"|\" * 3\n",
    "\n",
    "    config['model'] = {'stage_layer':stage_layer,'stage_ratio': stage_ratio ,'with_maxpool':with_maxpool,'op_type':op_type,\"layer_num_max\":layer_num_max}\n",
    "    config['train'] = {'start_epoch': 0, \"epochs\": 300, \"warmup_epochs\": 20, \"warmup_lr\": 1.0e-2, \n",
    "                    \"lr_scheduler\":{\"min_lr\":1.0e-5,\"decay_epochs\":30,\"decay_rate\":0.1},\"batch_size\":256,\"loss\":\"crossentropy\",\"workers\":4,\n",
    "                    \"optimizer\":{\"name\":\"SGD\",\"base_lr\":0.01,\"repvgg_lr\":0.001,\"momentum\":0.9,\"eps\":1.0e-8,\"betas\":[0.9,0.999],\"weight_decay_param\":{\"base_decay\":5e-4,\"repvgg_decay\":0.05,\"echo\":False}},\n",
    "                    \"label_smoothing\":0.1,\"clip_grad\":0.0,\"ema_alpha\":0.0,\"ema_update_period\":8,\"use_l2_norm\":True,\n",
    "                    \"no_weight_decay\": \"rbr_dense\"}\n",
    "    config['val'] = {\"batch_size\":128,\"workers\":4} \n",
    "    config['output'] = {\"print_freq\":100,\"epoch_print_freq\":1,\"save_freq\":20,\n",
    "                        \"dir\": os.path.join(\"./log\",dir), \"name\": str(n)}\n",
    "    save_path = os.path.join(\"./config\",dir+'.yaml')\n",
    "    md_fmts.append(md_fmt)\n",
    "    savedirs.append(save_path)\n",
    "    save_dict_to_yaml(config,save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for md in md_fmts:\n",
    "#     print(md)\n",
    "# for savedir in savedirs:\n",
    "#     print(\"python3 train.py --config ./config/\" + savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|99|2-1-5-1|0.625-1.0-0.5-0.75|vgg-vgg-vgg-vgg-repvgg|True-False-False-True|||'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_fmt = \"|\"+ str(n)+'|' + str(stage_layer[0]) + '-' + str(stage_layer[1]) + '-' + str(stage_layer[2]) + '-' + str(stage_layer[3]) + '|' \\\n",
    "        + str(stage_ratio[0]) + '-' + str(stage_ratio[1]) + '-' + str(stage_ratio[2]) + '-' + str(stage_ratio[3]) + \\\n",
    "            '|' + str(op_type[0]) + '-' + str(op_type[1]) + '-' + str(op_type[2]) + '-' + str(op_type[3]) + '-' + str(op_type[4]) + '|' + \\\n",
    "                str(with_maxpool[0]) + '-' + str(with_maxpool[1]) + '-' + str(with_maxpool[2]) + '-' + str(with_maxpool[3]) + \"|\" * 3\n",
    "md_fmt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# names = os.listdir(\"./config/\")\n",
    "# names.sort()\n",
    "# for num in range(0,12):\n",
    "#     print(\"python3 train.py --config ./config/\"+names[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "names = os.listdir(\"./config/\")\n",
    "    # print(name)\n",
    "names.sort()\n",
    "for num in range(0,100):\n",
    "    # print(names[num])\n",
    "    name = names[num].replace(\".yaml\",\"\").split(\"-\")\n",
    "    \n",
    "    name.remove(name[1])\n",
    "    name.remove(name[2])\n",
    "    name.remove(name[3])\n",
    "    name.remove(name[4])\n",
    "    for i in range(len(name)):\n",
    "        name[i] = name[i].replace(\"_\",\"-\")\n",
    "    # print(name)\n",
    "    # print(\"|\"+\"|\".join(name)+\"|\"*3)\n",
    "    # for n in names[0].split(\"-\"):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练自动生成yaml\n",
    "# loss acc train val\n",
    "# 存test最好的\n",
    "# loss crossentropy\n",
    "# SGD\n",
    "# lr 0.1，0.001（repvgg\n",
    "# weight decay 0.005，0.05（repvgg\n",
    "# 直接量化repvgg， repvgg block 的量化，需要考虑lr和weight decay\n",
    "\n",
    "# 随机生成模型配置，训\n",
    "# 得到小数据集\n",
    "# acc predictor， 是配置 （nas + predictor\n",
    "# 搜索算法 RL EL （chip memory 约束项，要提acc，降latency\n",
    "# 硬件性能 \n",
    "# hardware performance \n",
    "# 硬件结构固定，公式计算硬件性能\n",
    "# few-shot learning\n",
    "# meta-learning\n",
    "# 搜索考虑memory\n",
    "\n",
    "# 1. search space\n",
    "# 2. 重点是predictor\n",
    "# 3. \n",
    "\n",
    "# random genea\n",
    "# 进一步， 搜索对不同硬件都高效的网络\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_yaml_to_dict(yaml_path: str):\n",
    "#     with open(yaml_path) as file:\n",
    "#         dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "#         return dict_value\n",
    "cfg = read_yaml_to_dict(\"/home/qhy/Reserach/AICAS/config/stage-2_3_12_1-ratio-0.625_0.125_0.125_0.75-op-vgg_vgg_vgg_vgg-max-False_True_True_False.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_lr': 0.1,\n",
       " 'betas': [0.9, 0.999],\n",
       " 'eps': 1e-08,\n",
       " 'momentum': 0.9,\n",
       " 'name': 'SGD',\n",
       " 'repvgg_lr': 0.001,\n",
       " 'weight_decay_param': {'base_decay': 0.005,\n",
       "  'echo': False,\n",
       "  'repvgg_decay': 0.05}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['train'][\"optimizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qhy/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(stage_num,layer_num, channel_num_in , channel_num_out, op_type, with_maxpool):\n",
    "    channel_nums_in = [channel_num_in]+[channel_num_out]*(layer_num-1)\n",
    "    layers = []\n",
    "    if with_maxpool == True:\n",
    "        layers.append((\"maxpool\",nn.MaxPool2d(2,2)))\n",
    "        if op_type == 'vgg':\n",
    "            layers.append((\"stage_{}_0_vgg\".format(stage_num),VGGBlock(channel_num_in,channel_num_out,kernel_size=3,stride=1)))\n",
    "            layers += [(\"stage_{}_{}_vgg\".format(stage_num,i),VGGBlock(channel_num_out,channel_num_out,3)) for i in range(1,layer_num)]\n",
    "        else:\n",
    "            layers.append((\"stage_{}_0_repvgg\".format(stage_num),RepVGGBlock(channel_num_in,channel_num_out,kernel_size=3,stride=1,padding=1)))\n",
    "            layers += [(\"stage_{}_{}_repvgg\".format(stage_num,i),RepVGGBlock(channel_num_out,channel_num_out,kernel_size=3,stride=1,padding=1)) for i in range(1,layer_num)]\n",
    "            \n",
    "    else :\n",
    "        if op_type == 'vgg':\n",
    "            layers.append((\"stage_{}_0_vgg\".format(stage_num),VGGBlock(channel_num_in,channel_num_out,kernel_size=3,stride=2)))\n",
    "            layers += [(\"stage_{}_{}_vgg\".format(stage_num,i),VGGBlock(channel_num_out,channel_num_out,3)) for i in range(1,layer_num)]\n",
    "        else:\n",
    "            layers.append((\"stage_{}_0_repvgg\".format(stage_num),RepVGGBlock(channel_num_in,channel_num_out,kernel_size=3,stride=2))) \n",
    "            layers += [(\"stage_{}_{}_repvgg\".format(stage_num,i),RepVGGBlock(channel_num_out,channel_num_out,kernel_size=3,stride=1,padding=1)) for i in range(1,layer_num)]\n",
    "    return nn.Sequential(OrderedDict(layers))\n",
    "            \n",
    "        \n",
    "# make_layer(1,3,3,6,'repvgg',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_num_max': [64, 128, 256, 512],\n",
       " 'op_type': ['repvgg', 'vgg', 'vgg', 'repvgg'],\n",
       " 'stage_layer': [1, 1, 7, 1],\n",
       " 'stage_ratio': [0.625, 0.75, 0.125, 0.875],\n",
       " 'with_maxpool': [True, False, True, True]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,config,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.stage_0 = make_layer(0,config[\"model\"][\"stage_layer\"][0],3,int(config[\"model\"][\"layer_num_max\"][0]*config[\"model\"][\"stage_ratio\"][0]),config[\"model\"][\"op_type\"][0],config[\"model\"][\"with_maxpool\"][0])\n",
    "        self.stage_1 = make_layer(1,config[\"model\"][\"stage_layer\"][1],int(config[\"model\"][\"layer_num_max\"][0]*config[\"model\"][\"stage_ratio\"][0]),int(config[\"model\"][\"layer_num_max\"][1]*config[\"model\"][\"stage_ratio\"][1]),config[\"model\"][\"op_type\"][1],config[\"model\"][\"with_maxpool\"][1])\n",
    "        self.stage_2 = make_layer(2,config[\"model\"][\"stage_layer\"][2],int(config[\"model\"][\"layer_num_max\"][1]*config[\"model\"][\"stage_ratio\"][1]),int(config[\"model\"][\"layer_num_max\"][2]*config[\"model\"][\"stage_ratio\"][2]),config[\"model\"][\"op_type\"][2],config[\"model\"][\"with_maxpool\"][2])\n",
    "        self.stage_3 = make_layer(3,config[\"model\"][\"stage_layer\"][3],int(config[\"model\"][\"layer_num_max\"][2]*config[\"model\"][\"stage_ratio\"][2]),int(config[\"model\"][\"layer_num_max\"][3]*config[\"model\"][\"stage_ratio\"][3]),config[\"model\"][\"op_type\"][3],config[\"model\"][\"with_maxpool\"][3])\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(int(config[\"model\"][\"layer_num_max\"][3]*config[\"model\"][\"stage_ratio\"][3]), num_classes)\n",
    "    def forward(self, input):\n",
    "        out = self.stage_0(input)\n",
    "        for stage in (self.stage_1, self.stage_2,  self.stage_3):\n",
    "            for block in stage:\n",
    "                out = block(out)\n",
    "        out = self.gap(out)\n",
    "        out = self.linear(out.view(out.size(0), -1))\n",
    "        return out\n",
    "n = Net(cfg,10)\n",
    "inp = torch.randn(1,3,32,32)\n",
    "o = n(inp)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "repvgg_stage = torch.nn.ModuleList()\n",
    "net = n\n",
    "for stage in (net.stage_0,net.stage_1,net.stage_2,net.stage_3):\n",
    "    for name, module in stage._modules.items():\n",
    "        if \"repvgg\" in name:\n",
    "            repvgg_stage.append(stage)\n",
    "repvgg_stage_params = list(map(id,repvgg_stage.parameters()))\n",
    "base_stage_param = filter(lambda p:id(p) not in repvgg_stage_params,net.parameters())\n",
    "optimizer = torch.optim.SGD([{'params':base_stage_param,'lr':0.1,\"weight_decay\":0.005},\n",
    "                             {'params':repvgg_stage.parameters(),'lr':0.001,\"weight_decay\":0.05}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in (n.stage_0,n.stage_1,n.stage_2,n.stage_3):\n",
    "    for name, module in stage._modules.items():\n",
    "        if \"repvgg\" in name:\n",
    "            module.switch_to_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(n.stage_0.stage_0_0_repvgg)\n",
    "# for i in n.stage_0.modules():\n",
    "#     print(i.name)\n",
    "#     break\n",
    "# for name, module in n.stage_0._modules.items():\n",
    "#     print(module)\n",
    "# for name, module in n.stage_0._modules.items():\n",
    "#     # print(\"名称:{}\".format(name))\n",
    "#     if \"repvgg\" in name:\n",
    "#         module.switch_to_deploy()\n",
    "# for name, module in n.stage_0._modules.items():\n",
    "#     # print(name)\n",
    "#     print(module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.stage_0.stage_0_0.switch_to_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def switch_to_deploy(self):\n",
    "    #     if hasattr(self, 'rbr_reparam'):\n",
    "    #         return\n",
    "    #     kernel, bias = self.get_equivalent_kernel_bias()\n",
    "    #     self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels, kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride, padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "    #     self.rbr_reparam.weight.data = kernel\n",
    "    #     self.rbr_reparam.bias.data = bias\n",
    "    #     self.__delattr__('rbr_dense')\n",
    "    #     self.__delattr__('rbr_1x1')\n",
    "    #     if hasattr(self, 'rbr_identity'):\n",
    "    #         self.__delattr__('rbr_identity')\n",
    "    #     if hasattr(self, 'id_tensor'):\n",
    "    #         self.__delattr__('id_tensor')\n",
    "    #     self.deploy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict \n",
    "def VGGBlock(in_channels, out_channels, kernel_size, stride = 1, padding = 1, dilation = 1, groups = 1, padding_mode = 'zeros'):\n",
    "    conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1 ,dilation = 1, groups = 1, padding_mode = 'zeros')\n",
    "    layers = nn.Sequential(OrderedDict([\n",
    "      (\"conv\",conv2d),\n",
    "      (\"bn\",nn.BatchNorm2d(out_channels)),\n",
    "      (\"relu\",nn.ReLU(inplace=True))]))\n",
    "    return layers\n",
    "VGGBlock(3,6,3)\n",
    "\n",
    "# conv_module = nn.Sequential(OrderedDict([\n",
    "#           ('conv1', nn.Conv2d(1,20,5)),\n",
    "#           ('relu1', nn.ReLU()),\n",
    "#           ('conv2', nn.Conv2d(20,64,5)),\n",
    "#           ('relu2', nn.ReLU())\n",
    "#         ]))\n",
    "# conv_module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nn.Sequential(OrderedDict([\n",
    "#     (\"stage_0_{}\".format(i),VGGBlock(3,6,3)) for i in range(3)\n",
    "    \n",
    "#     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Sequential(OrderedDict([\n",
    "    (\"stage_0_1\",VGGBlock(3,6,3)),\n",
    "    (\"stage_0_2\",VGGBlock(6,6,3)),\n",
    "    ]))\n",
    "b = torch.randn(1,3,32,32)\n",
    "c = a(b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage = nn.Sequential()\n",
    "# stage.append(OrderedDict([(\"stage_0\",VGGBlock(3,6,3))]))\n",
    "# # stage.append(VGGBlock(6,6,3))\n",
    "# stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Naive RepVGG without SE module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.non_linearity = nn.ReLU()\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.rbr_identity = nn.BatchNorm2d(num_features=out_channels) if out_channels == in_channels and stride == 1 else None\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.non_linearity(self.rbr_reparam(inputs))\n",
    "        \n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "        \n",
    "        return self.non_linearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
    "    \n",
    "    def get_custom_L2(self):\n",
    "        K3 = self.rbr_dense.conv.weight\n",
    "        K1 = self.rbr_1x1.conv.weight\n",
    "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "\n",
    "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
    "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
    "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
    "        return l2_loss_eq_kernel + l2_loss_circle\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])\n",
    "    \n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
    "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels, kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride, padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "        self.rbr_reparam.weight.data = kernel\n",
    "        self.rbr_reparam.bias.data = bias\n",
    "        self.__delattr__('rbr_dense')\n",
    "        self.__delattr__('rbr_1x1')\n",
    "        if hasattr(self, 'rbr_identity'):\n",
    "            self.__delattr__('rbr_identity')\n",
    "        if hasattr(self, 'id_tensor'):\n",
    "            self.__delattr__('id_tensor')\n",
    "        self.deploy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.stage_1 = RepVGGBlock(3,3,3,padding=1)\n",
    "        self.stage_2 = VGGBlock(3,6,3)\n",
    "    def forward(self, input):\n",
    "        out = self.stage_1(input)\n",
    "        out = self.stage_2(out)\n",
    "        return out\n",
    "n = Net()\n",
    "inp = torch.randn(1,3,32,32)\n",
    "o = n(inp)\n",
    "print(o.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepVGGBlock(\n",
       "  (non_linearity): ReLU()\n",
       "  (rbr_dense): Sequential(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (rbr_1x1): Sequential(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RepVGGBlock(3,64,3,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding == 1 是为了保证尺寸不变\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    '''\n",
    "    根据配置表，返回模型层列表\n",
    "    '''\n",
    "    layers = [] # 层列表初始化\n",
    "\n",
    "    in_channels = 3 # 输入3通道图像\n",
    "\n",
    "    # 遍历配置列表\n",
    "    for v in cfg:\n",
    "        if v == 'M': # 添加池化层\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else: # 添加卷积层\n",
    "\n",
    "            # 3×3 卷积\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "\n",
    "            # 卷积-->批归一化（可选）--> ReLU激活\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "\n",
    "            # 通道数方面，下一层输入即为本层输出\n",
    "            in_channels = v\n",
    "\n",
    "    # 以sequencial类型返回模型层列表\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# make_layers(cfgs['A'], batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG通用网络模型\n",
    "    输入features为网络的特征提取部分网络层列表\n",
    "    分类数为 1000\n",
    "    '''\n",
    "    def __init__(self, features, dataset = 'cifar10', init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "            \n",
    "        # 特征提取部分\n",
    "        self.features = features\n",
    "\n",
    "        # 自适应平均池化，特征图池化到 7×7 大小\n",
    "        # 为了让下一步的flatten得到一个固定长度的向量\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        # 分类部分\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),   # 512*7*7 --> 4096\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),          # 4096 --> 4096\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),   # 4096 --> 1000\n",
    "        )\n",
    "\n",
    "        # 权重初始化\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 特征提取\n",
    "        x = self.features(x)\n",
    "        # 自适应平均池化\n",
    "        x = self.avgpool(x)\n",
    "        # 特征图展平成向量\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 分类器分类输出\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        '''\n",
    "        权重初始化\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # 卷积层使用 kaimming 初始化\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                # 偏置初始化为0\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # 批归一化层权重初始化为1 \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            # 全连接层权重初始化\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 1, 1])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# models.vgg19_bn(pretrained=True)\n",
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "        self.gap = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        print(out.shape)\n",
    "        out = self.gap(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        # layers += []\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG16')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VGG('VGG16')\n",
    "net\n",
    "# {64->64:1,max,64->128:1,128->128:1,max,128->256:1,256->256:2,max,256->512:1,512:5,max,gap,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "# config = EasyDict(yaml.full_load(open()))\n",
    "import numpy as np\n",
    "from random import choices \n",
    "\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [2, 4, 14, 2]\n",
    "# the channel number of each layer\n",
    "layer_num_max = [64, 128, 256, 512, 512]\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "maxpool = [True,False]\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "datasets = [\"cifar10\",\"cifar100\"]\n",
    "\n",
    "\n",
    "# operation \n",
    "# dataset = choices(datasets)[0]\n",
    "# class_num = 10 if dataset == \"cifar10\" else 100\n",
    "import yaml\n",
    "import os \n",
    "import collections\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    \"\"\"dict保存为yaml\"\"\"\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    " \n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "savedirs = []\n",
    "md_fmts = []\n",
    "config = {}\n",
    "# 随机生成模型\n",
    "stage_layer = [2,2,3,3,3]\n",
    "# stage_layer\n",
    "stage_ratio = [1.0,1.0,1.0,1.0,1.0]\n",
    "# stage_ratio\n",
    "with_maxpool = [False,True,True,True,True]\n",
    "# with_maxpool\n",
    "op_type = ['vgg','vgg','vgg','vgg','vgg']\n",
    "n = \"vgg16\"\n",
    "dir = str(n)+'-stage-' + str(stage_layer[0]) + '_' + str(stage_layer[1]) + '_' + str(stage_layer[2]) + '_' + str(stage_layer[3]) + '_' + str(stage_layer[4]) + '-ratio-' \\\n",
    "    + str(stage_ratio[0]) + '_' + str(stage_ratio[1]) + '_' + str(stage_ratio[2]) + '_' + str(stage_ratio[3]) + '_' + str(stage_ratio[4]) +  \\\n",
    "        '-op-' + str(op_type[0]) + '_' + str(op_type[1]) + '_' + str(op_type[2]) + '_' + str(op_type[3]) + '_' + str(op_type[4]) + '-max-' + \\\n",
    "            str(with_maxpool[0]) + '_' + str(with_maxpool[1]) + '_' + str(with_maxpool[2]) + '_' + str(with_maxpool[3]) + '_' + str(with_maxpool[4])\n",
    "md_fmt = \"|\"+ str(n)+'|' + str(stage_layer[0]) + '-' + str(stage_layer[1]) + '-' + str(stage_layer[2]) + '-' + str(stage_layer[3]) + '|' + str(stage_layer[4]) + '|'\\\n",
    "    + str(stage_ratio[0]) + '-' + str(stage_ratio[1]) + '-' + str(stage_ratio[2]) + '-' + str(stage_ratio[3]) + '-' + str(stage_ratio[4]) + \\\n",
    "        '|' + str(op_type[0]) + '-' + str(op_type[1]) + '-' + str(op_type[2]) + '-' + str(op_type[3]) + '-' + str(op_type[4]) + '|' + \\\n",
    "            str(with_maxpool[0]) + '-' + str(with_maxpool[1]) + '-' + str(with_maxpool[2]) + '-' + str(with_maxpool[3]) + '-' + str(with_maxpool[4]) + \"|\" * 3\n",
    "\n",
    "config['model'] = {'stage_layer':stage_layer,'stage_ratio': stage_ratio ,'with_maxpool':with_maxpool,'op_type':op_type,\"layer_num_max\":layer_num_max}\n",
    "config['train'] = {'start_epoch': 0, \"epochs\": 300, \"warmup_epochs\": 20, \"warmup_lr\": 1.0e-2, \n",
    "                \"lr_scheduler\":{\"min_lr\":1.0e-5,\"decay_epochs\":30,\"decay_rate\":0.1},\"batch_size\":256,\"loss\":\"crossentropy\",\"workers\":4,\n",
    "                \"optimizer\":{\"name\":\"SGD\",\"base_lr\":0.01,\"repvgg_lr\":0.001,\"momentum\":0.9,\"eps\":1.0e-8,\"betas\":[0.9,0.999],\"weight_decay_param\":{\"base_decay\":5e-4,\"repvgg_decay\":0.05,\"echo\":False}},\n",
    "                \"label_smoothing\":0.1,\"clip_grad\":0.0,\"ema_alpha\":0.0,\"ema_update_period\":8,\"use_l2_norm\":True,\n",
    "                \"no_weight_decay\": \"rbr_dense\"}\n",
    "config['val'] = {\"batch_size\":128,\"workers\":4} \n",
    "config['output'] = {\"print_freq\":100,\"epoch_print_freq\":1,\"save_freq\":20,\n",
    "                    \"dir\": os.path.join(\"./log\",dir), \"name\": str(n)}\n",
    "save_path = os.path.join(\"./config\",dir+'.yaml')\n",
    "md_fmts.append(md_fmt)\n",
    "savedirs.append(save_path)\n",
    "save_dict_to_yaml(config,save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import model\n",
    "# m = model.Net(config=config,num_classes=10)\n",
    "# # x = torch.randn(2,3,32,32)\n",
    "# # m(x)\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import model\n",
    "# m = model.Net(config=config,num_classes=10)\n",
    "# x = torch.randn(2,3,32,32)\n",
    "# m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# config = EasyDict(yaml.full_load(open()))\n",
    "import numpy as np\n",
    "from random import choices \n",
    "import torch\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [1, 2, 4, 14, 2]\n",
    "# the channel number of each layer\n",
    "layer_num_max = [64, 64, 128, 256, 512]\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "maxpool = [True,False]\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "datasets = [\"cifar10\",\"cifar100\"]\n",
    "\n",
    "\n",
    "# operation \n",
    "# dataset = choices(datasets)[0]\n",
    "# class_num = 10 if dataset == \"cifar10\" else 100\n",
    "import yaml\n",
    "import os \n",
    "import collections\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    \"\"\"dict保存为yaml\"\"\"\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    " \n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "savedirs = []\n",
    "md_fmts = []\n",
    "config = {}\n",
    "# 随机生成模型\n",
    "stage_layer = [2,2,3,3,3]\n",
    "# stage_layer\n",
    "stage_ratio = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "# stage_ratio\n",
    "with_maxpool = [False,False,False,False,False]\n",
    "# with_maxpool\n",
    "op_type = ['repvgg','repvgg','repvgg','repvgg','repvgg']\n",
    "n = \"repvgg-A0\"\n",
    "dir = str(n)+'-stage-' + str(stage_layer[0]) + '_' + str(stage_layer[1]) + '_' + str(stage_layer[2]) + '_' + str(stage_layer[3]) + '_' + str(stage_layer[4]) + '-ratio-' \\\n",
    "    + str(stage_ratio[0]) + '_' + str(stage_ratio[1]) + '_' + str(stage_ratio[2]) + '_' + str(stage_ratio[3]) + '_' + str(stage_ratio[4]) +  \\\n",
    "        '-op-' + str(op_type[0]) + '_' + str(op_type[1]) + '_' + str(op_type[2]) + '_' + str(op_type[3]) + '_' + str(op_type[4]) + '-max-' + \\\n",
    "            str(with_maxpool[0]) + '_' + str(with_maxpool[1]) + '_' + str(with_maxpool[2]) + '_' + str(with_maxpool[3]) + '_' + str(with_maxpool[4])\n",
    "md_fmt = \"|\"+ str(n)+'|' + str(stage_layer[0]) + '-' + str(stage_layer[1]) + '-' + str(stage_layer[2]) + '-' + str(stage_layer[3]) + '|' + str(stage_layer[4]) + '|'\\\n",
    "    + str(stage_ratio[0]) + '-' + str(stage_ratio[1]) + '-' + str(stage_ratio[2]) + '-' + str(stage_ratio[3]) + '-' + str(stage_ratio[4]) + \\\n",
    "        '|' + str(op_type[0]) + '-' + str(op_type[1]) + '-' + str(op_type[2]) + '-' + str(op_type[3]) + '-' + str(op_type[4]) + '|' + \\\n",
    "            str(with_maxpool[0]) + '-' + str(with_maxpool[1]) + '-' + str(with_maxpool[2]) + '-' + str(with_maxpool[3]) + '-' + str(with_maxpool[4]) + \"|\" * 3\n",
    "\n",
    "config['model'] = {'stage_layer':stage_layer,'stage_ratio': stage_ratio ,'with_maxpool':with_maxpool,'op_type':op_type,\"layer_num_max\":layer_num_max}\n",
    "config['train'] = {'start_epoch': 0, \"epochs\": 300, \"warmup_epochs\": 20, \"warmup_lr\": 1.0e-2, \n",
    "                \"lr_scheduler\":{\"min_lr\":1.0e-5,\"decay_epochs\":30,\"decay_rate\":0.1},\"batch_size\":256,\"loss\":\"crossentropy\",\"workers\":4,\n",
    "                \"optimizer\":{\"name\":\"SGD\",\"base_lr\":0.01,\"repvgg_lr\":0.1,\"momentum\":0.9,\"eps\":1.0e-8,\"betas\":[0.9,0.999],\"weight_decay_param\":{\"base_decay\":5e-4,\"repvgg_decay\":1e-4,\"echo\":False}},\n",
    "                \"label_smoothing\":0.1,\"clip_grad\":0.0,\"ema_alpha\":0.0,\"ema_update_period\":8,\"use_l2_norm\":True,\n",
    "                \"no_weight_decay\": \"rbr_dense\"}\n",
    "config['val'] = {\"batch_size\":128,\"workers\":4} \n",
    "config['output'] = {\"print_freq\":100,\"epoch_print_freq\":1,\"save_freq\":20,\n",
    "                    \"dir\": os.path.join(\"./log\",dir), \"name\": str(n)}\n",
    "save_path = os.path.join(\"./config\",dir+'.yaml')\n",
    "md_fmts.append(md_fmt)\n",
    "savedirs.append(save_path)\n",
    "save_dict_to_yaml(config,save_path)\n",
    "\n",
    "from models import model\n",
    "m = model.Net(config=config,num_classes=10)\n",
    "x = torch.randn(2,3,32,32)\n",
    "m(x).shape\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "# from se_block import SEBlock\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    return result\n",
    "\n",
    "class RepVGGBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "\n",
    "        if use_se:\n",
    "            #   Note that RepVGG-D2se uses SE before nonlinearity. But RepVGGplus models uses SE after nonlinearity.\n",
    "            self.se = SEBlock(out_channels, internal_neurons=out_channels // 16)\n",
    "        else:\n",
    "            self.se = nn.Identity()\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "\n",
    "        else:\n",
    "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
    "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n",
    "\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "\n",
    "        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n",
    "\n",
    "\n",
    "    #   Optional. This may improve the accuracy and facilitates quantization in some cases.\n",
    "    #   1.  Cancel the original weight decay on rbr_dense.conv.weight and rbr_1x1.conv.weight.\n",
    "    #   2.  Use like this.\n",
    "    #       loss = criterion(....)\n",
    "    #       for every RepVGGBlock blk:\n",
    "    #           loss += weight_decay_coefficient * 0.5 * blk.get_cust_L2()\n",
    "    #       optimizer.zero_grad()\n",
    "    #       loss.backward()\n",
    "    def get_custom_L2(self):\n",
    "        K3 = self.rbr_dense.conv.weight\n",
    "        K1 = self.rbr_1x1.conv.weight\n",
    "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "\n",
    "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
    "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
    "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
    "        return l2_loss_eq_kernel + l2_loss_circle\n",
    "\n",
    "\n",
    "\n",
    "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
    "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
    "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
    "#   May be useful for quantization or pruning.\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
    "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,\n",
    "                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n",
    "                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "        self.rbr_reparam.weight.data = kernel\n",
    "        self.rbr_reparam.bias.data = bias\n",
    "        self.__delattr__('rbr_dense')\n",
    "        self.__delattr__('rbr_1x1')\n",
    "        if hasattr(self, 'rbr_identity'):\n",
    "            self.__delattr__('rbr_identity')\n",
    "        if hasattr(self, 'id_tensor'):\n",
    "            self.__delattr__('id_tensor')\n",
    "        self.deploy = True\n",
    "\n",
    "\n",
    "\n",
    "class RepVGG(nn.Module):\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False, use_se=False, use_checkpoint=False):\n",
    "        super(RepVGG, self).__init__()\n",
    "        assert len(width_multiplier) == 4\n",
    "        self.deploy = deploy\n",
    "        self.override_groups_map = override_groups_map or dict()\n",
    "        assert 0 not in self.override_groups_map\n",
    "        self.use_se = use_se\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
    "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy, use_se=self.use_se)\n",
    "        self.cur_layer_idx = 1\n",
    "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
    "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
    "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
    "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
    "\n",
    "    def _make_stage(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
    "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
    "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy, use_se=self.use_se))\n",
    "            self.in_planes = planes\n",
    "            self.cur_layer_idx += 1\n",
    "        return nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stage0(x)\n",
    "        for stage in (self.stage1, self.stage2, self.stage3, self.stage4):\n",
    "            for block in stage:\n",
    "                if self.use_checkpoint:\n",
    "                    out = checkpoint.checkpoint(block, out)\n",
    "                else:\n",
    "                    out = block(out)\n",
    "        out = self.gap(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]\n",
    "g2_map = {l: 2 for l in optional_groupwise_layers}\n",
    "g4_map = {l: 4 for l in optional_groupwise_layers}\n",
    "\n",
    "def create_RepVGG_A0(deploy=False, use_checkpoint=False):\n",
    "    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000,\n",
    "                  width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy, use_checkpoint=use_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mo = create_RepVGG_A0()\n",
    "# print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "====================================================================================================\n",
      "|690|1-4-3-1-3|0.25-0.25-0.25-1.0-0.5|vgg-vgg-repvgg-repvgg-repvgg|False-True-True-False-True|None-avgpool-avgpool-None-avgpool-avgpool|cifar10|13.89|2.09||\n",
      "|691|2-4-3-2-1|0.25-0.25-0.625-0.5-0.125|repvgg-vgg-repvgg-repvgg-vgg|False-True-False-True-True|None-maxpool-None-maxpool-avgpool-maxpool|cifar10|19.64|0.5||\n",
      "|692|2-2-2-5-2|1.0-1.0-0.75-1.0-0.5|vgg-vgg-repvgg-vgg-vgg|False-True-False-True-False|None-avgpool-None-maxpool-None-avgpool|cifar10|115.12|4.04||\n",
      "|693|2-1-3-4-3|0.75-0.75-0.5-0.375-0.25|repvgg-vgg-vgg-vgg-vgg|False-False-True-True-True|None-None-avgpool-maxpool-maxpool-avgpool|cifar10|44.47|0.86||\n",
      "|694|2-2-5-8-2|0.125-0.125-0.375-0.375-0.375|repvgg-repvgg-repvgg-vgg-vgg|False-True-True-True-True|None-maxpool-maxpool-maxpool-avgpool-avgpool|cifar10|19.76|1.22||\n",
      "|695|2-1-5-2-1|0.875-0.875-0.75-0.75-0.625|vgg-vgg-repvgg-vgg-vgg|False-True-True-True-False|None-maxpool-maxpool-avgpool-None-avgpool|cifar10|75.78|1.54||\n",
      "|696|2-3-5-1-3|0.625-0.625-0.875-1.0-0.5|vgg-vgg-repvgg-vgg-repvgg|False-True-True-True-True|None-maxpool-maxpool-maxpool-maxpool-avgpool|cifar10|74.79|2.84||\n",
      "|697|1-1-2-2-1|0.75-0.75-0.75-0.875-0.625|repvgg-vgg-vgg-vgg-repvgg|False-True-True-False-True|None-maxpool-maxpool-None-maxpool-None|cifar10|28.47|1.52||\n",
      "|698|2-1-6-6-4|0.375-0.375-0.5-0.5-0.75|repvgg-vgg-repvgg-vgg-vgg|False-True-True-False-True|None-maxpool-avgpool-None-maxpool-maxpool|cifar10|53.6|5.48||\n",
      "|699|1-1-2-4-3|0.875-0.875-0.125-1.0-1.0|repvgg-vgg-repvgg-repvgg-vgg|False-False-False-False-True|None-None-None-None-avgpool-maxpool|cifar10|66.14|7.96||\n",
      "====================================================================================================\n",
      "python3 train.py --config ./config/690-stage-1_4_3_1_3-ratio-0.25_0.25_0.25_1.0_0.5-op-vgg_vgg_repvgg_repvgg_repvgg-max-False_True_True_False_True-pool_type-None_avgpool_avgpool_None_avgpool_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/691-stage-2_4_3_2_1-ratio-0.25_0.25_0.625_0.5_0.125-op-repvgg_vgg_repvgg_repvgg_vgg-max-False_True_False_True_True-pool_type-None_maxpool_None_maxpool_avgpool_maxpool-cifar10.yaml\n",
      "python3 train.py --config ./config/692-stage-2_2_2_5_2-ratio-1.0_1.0_0.75_1.0_0.5-op-vgg_vgg_repvgg_vgg_vgg-max-False_True_False_True_False-pool_type-None_avgpool_None_maxpool_None_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/693-stage-2_1_3_4_3-ratio-0.75_0.75_0.5_0.375_0.25-op-repvgg_vgg_vgg_vgg_vgg-max-False_False_True_True_True-pool_type-None_None_avgpool_maxpool_maxpool_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/694-stage-2_2_5_8_2-ratio-0.125_0.125_0.375_0.375_0.375-op-repvgg_repvgg_repvgg_vgg_vgg-max-False_True_True_True_True-pool_type-None_maxpool_maxpool_maxpool_avgpool_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/695-stage-2_1_5_2_1-ratio-0.875_0.875_0.75_0.75_0.625-op-vgg_vgg_repvgg_vgg_vgg-max-False_True_True_True_False-pool_type-None_maxpool_maxpool_avgpool_None_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/696-stage-2_3_5_1_3-ratio-0.625_0.625_0.875_1.0_0.5-op-vgg_vgg_repvgg_vgg_repvgg-max-False_True_True_True_True-pool_type-None_maxpool_maxpool_maxpool_maxpool_avgpool-cifar10.yaml\n",
      "python3 train.py --config ./config/697-stage-1_1_2_2_1-ratio-0.75_0.75_0.75_0.875_0.625-op-repvgg_vgg_vgg_vgg_repvgg-max-False_True_True_False_True-pool_type-None_maxpool_maxpool_None_maxpool_None-cifar10.yaml\n",
      "python3 train.py --config ./config/698-stage-2_1_6_6_4-ratio-0.375_0.375_0.5_0.5_0.75-op-repvgg_vgg_repvgg_vgg_vgg-max-False_True_True_False_True-pool_type-None_maxpool_avgpool_None_maxpool_maxpool-cifar10.yaml\n",
      "python3 train.py --config ./config/699-stage-1_1_2_4_3-ratio-0.875_0.875_0.125_1.0_1.0-op-repvgg_vgg_repvgg_repvgg_vgg-max-False_False_False_False_True-pool_type-None_None_None_None_avgpool_maxpool-cifar10.yaml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import choices \n",
    "\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [2, 4, 6, 8, 4]\n",
    "# the channel number of each layer\n",
    "layer_num_max = [64, 64, 128, 256, 512]\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "pool_types = [True,False]\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "datasets = [\"cifar10\"]\n",
    "pool_type = [\"maxpool\",\"avgpool\"]\n",
    "# operation \n",
    "# dataset = choices(datasets)[0]\n",
    "# class_num = 10 if dataset == \"cifar10\" else 100\n",
    "import yaml\n",
    "import torch\n",
    "import os \n",
    "import collections\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from thop import profile \n",
    "from models import model as M\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    \"\"\"dict保存为yaml\"\"\"\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    " \n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "savedirs = []\n",
    "md_fmts = []\n",
    "for n in range(690,700):\n",
    "    config = {}\n",
    "    pool = []\n",
    "    dataset = choices(datasets)[0]\n",
    "    # 随机生成模型\n",
    "    stage_layer = [choices(range(1,stage_layer_max[0]+1))[0],choices(range(1,stage_layer_max[1]+1))[0],choices(range(1,stage_layer_max[2]+1))[0],choices(range(1,stage_layer_max[3]+1))[0],choices(range(1,stage_layer_max[4]+1))[0]]\n",
    "    # stage_layer\n",
    "    ratio_1 = choices(ratio)[0]\n",
    "    stage_ratio = [ratio_1,ratio_1,choices(ratio)[0],choices(ratio)[0],choices(ratio)[0]]\n",
    "    # stage_ratio\n",
    "    with_pool = [False,choices(pool_types)[0],choices(pool_types)[0],choices(pool_types)[0],choices(pool_types)[0]]\n",
    "    # with_pool\n",
    "    op_type = [choices(op)[0],choices(op)[0],choices(op)[0],choices(op)[0],choices(op)[0]]\n",
    "    with_last_pool = choices(pool_types)[0]\n",
    "    \n",
    "        \n",
    "    for pool_or_not in with_pool:\n",
    "        if pool_or_not:\n",
    "             pool.append(choices(pool_type)[0])\n",
    "        else:\n",
    "            pool.append(None)\n",
    "    if with_last_pool:\n",
    "        pool.append(choices(pool_type)[0])\n",
    "    else:\n",
    "        pool.append(None)\n",
    "    dir = str(n)+'-stage-' + str(stage_layer[0]) + '_' + str(stage_layer[1]) + '_' + str(stage_layer[2]) + '_' + str(stage_layer[3]) + '_' + str(stage_layer[4]) + '-ratio-' \\\n",
    "    + str(stage_ratio[0]) + '_' + str(stage_ratio[1]) + '_' + str(stage_ratio[2]) + '_' + str(stage_ratio[3]) + '_' + str(stage_ratio[4]) +  \\\n",
    "        '-op-' + str(op_type[0]) + '_' + str(op_type[1]) + '_' + str(op_type[2]) + '_' + str(op_type[3]) + '_' + str(op_type[4]) + '-max-' + \\\n",
    "            str(with_pool[0]) + '_' + str(with_pool[1]) + '_' + str(with_pool[2]) + '_' + str(with_pool[3]) + '_' + str(with_pool[4]) + '-pool_type-'+\\\n",
    "                str(pool[0]) + '_' + str(pool[1]) + '_' + str(pool[2]) + '_' + str(pool[3]) + '_' + str(pool[4]) + '_' + str(pool[5]) + '-' + dataset\n",
    "    \n",
    "    config['dataset'] = dataset\n",
    "    config['model'] = {'stage_layer':stage_layer,'stage_ratio': stage_ratio ,'with_pool':with_pool,\"pool_type\":pool,'op_type':op_type,\"layer_num_max\":layer_num_max,\"with_last_pool\":with_last_pool}\n",
    "    config['train'] = {'start_epoch': 0, \"epochs\": 100, \"warmup_epochs\": 0, \"warmup_lr\": 1.0e-2, \n",
    "                    \"lr_scheduler\":{\"min_lr\":1.0e-5,\"decay_epochs\":30,\"decay_rate\":0.1},\"batch_size\":256,\"loss\":\"crossentropy\",\"workers\":4,\n",
    "                    \"optimizer\":{\"name\":\"SGD\",\"base_lr\":0.1,\"repvgg_lr\":0.1,\"momentum\":0.9,\"eps\":1.0e-8,\"betas\":[0.9,0.999],\"weight_decay_param\":{\"base_decay\":5e-4,\"repvgg_decay\":1e-4,\"echo\":False}},\n",
    "                    \"label_smoothing\":0.1,\"clip_grad\":0.0,\"ema_alpha\":0.0,\"ema_update_period\":8,\"use_l2_norm\":True,\n",
    "                    \"no_weight_decay\": [\"rbr_dense\",\"rbr_1x1\"]}\n",
    "    config['val'] = {\"batch_size\":128,\"workers\":4} \n",
    "    config['output'] = {\"print_freq\":100,\"epoch_print_freq\":1,\"save_freq\":20,\n",
    "                        \"dir\": os.path.join(\"./log\",dir), \"name\": str(n)}\n",
    "    \n",
    "    save_path = os.path.join(\"../config\",dir+'.yaml')\n",
    "    model = M.Net(config, 10)\n",
    "    input = torch.randn(1, 3, 32, 32)\n",
    "    flops,params = profile(model, inputs=(input, ))\n",
    "    md_fmt = \"|\"+ str(n)+'|' + str(stage_layer[0]) + '-' + str(stage_layer[1]) + '-' + str(stage_layer[2]) + '-' + str(stage_layer[3]) + '-' + str(stage_layer[4]) + '|'\\\n",
    "        + str(stage_ratio[0]) + '-' + str(stage_ratio[1]) + '-' + str(stage_ratio[2]) + '-' + str(stage_ratio[3]) + '-' + str(stage_ratio[4]) + \\\n",
    "            '|' + str(op_type[0]) + '-' + str(op_type[1]) + '-' + str(op_type[2]) + '-' + str(op_type[3]) + '-' + str(op_type[4]) + '|' + \\\n",
    "                str(with_pool[0]) + '-' + str(with_pool[1]) + '-' + str(with_pool[2]) + '-' + str(with_pool[3]) + '-' + str(with_pool[4]) + '|' +\\\n",
    "                str(pool[0]) + '-' + str(pool[1]) + '-' + str(pool[2]) + '-' + str(pool[3]) + '-' + str(pool[4]) + '-' + str(pool[5]) + '|' + dataset+ \"|\" + str(round(flops/1e6,2))+\"|\"+str(round(params/1e6,2))+\"|\"+\"|\"\n",
    "    md_fmts.append(md_fmt)\n",
    "    savedirs.append(save_path.replace(\"..\",\".\"))\n",
    "    save_dict_to_yaml(config,save_path)\n",
    "\n",
    "print(\"=\"*100)\n",
    "for md in md_fmts:\n",
    "    print(md)\n",
    "print(\"=\"*100)\n",
    "for savedir in savedirs:\n",
    "    print(\"python3 train.py --config \" + savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model\n",
    "m = model.Net(config=config,num_classes=10)\n",
    "# x = torch.randn(2,3,32,32)\n",
    "# m(x)\n",
    "from easydict import EasyDict\n",
    "config = EasyDict(yaml.full_load(open(\"./config/13-stage-1_1_2_12_1-ratio-0.5_0.5_0.75_0.5_0.75-op-vgg_vgg_vgg_repvgg_vgg-max-False_True_True_True_False.yaml\")))\n",
    "config.train.optimizer.weight_decay_param.echo = True\n",
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(config, model):\n",
    "    \"\"\"\n",
    "    Build optimizer, set weight decay of normalization to 0 by default.\n",
    "    \"\"\"\n",
    "    skip = {}\n",
    "    skip_keywords = {}\n",
    "    if hasattr(model, 'no_weight_decay'):\n",
    "        skip = model.no_weight_decay()\n",
    "    if hasattr(model, 'no_weight_decay_keywords'):\n",
    "        if config.train.use_l2_norm:\n",
    "            skip_keywords = model.no_weight_decay_keywords(\n",
    "                config.train.no_weight_decay)\n",
    "        else:\n",
    "            skip_keywords = model.no_weight_decay_keywords()\n",
    "    parameters = set_weight_decay(\n",
    "        model,\n",
    "        skip,\n",
    "        skip_keywords,\n",
    "        echo=config.train.optimizer.weight_decay_param.echo)\n",
    "    repvgg_stage = torch.nn.ModuleList()\n",
    "    for stage in (model.stage_0, model.stage_1, model.stage_2, model.stage_3):\n",
    "        for name, module in stage._modules.items():\n",
    "            if \"repvgg\" in name:\n",
    "                repvgg_stage.append(stage)\n",
    "    repvgg_stage_params = list(map(id, repvgg_stage.parameters()))\n",
    "    base_stage_param = filter(lambda p: id(p) not in repvgg_stage_params,\n",
    "                              model.parameters())\n",
    "\n",
    "    opt_lower = config.train.optimizer.name.lower()\n",
    "    optimizer = None\n",
    "    if opt_lower == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            [{\n",
    "                'params':\n",
    "                base_stage_param,\n",
    "                'lr':\n",
    "                config.train.optimizer.base_lr,\n",
    "                \"weight_decay\":\n",
    "                config.train.optimizer.weight_decay_param.base_decay\n",
    "            }, {\n",
    "                'params':\n",
    "                repvgg_stage.parameters(),\n",
    "                'lr':\n",
    "                config.train.optimizer.repvgg_lr,\n",
    "                \"weight_decay\":\n",
    "                config.train.optimizer.weight_decay_param.repvgg_decay\n",
    "            }],\n",
    "            momentum=config.train.optimizer.momentum,\n",
    "            nesterov=True,\n",
    "        )\n",
    "        # optimizer = optim.SGD(parameters, momentum=config.train.optimizer.momentum, nesterov=True,\n",
    "        #                       lr=config.train.optimizer.base_lr, weight_decay=config.train.optimizer.weight_decay_param.decay)\n",
    "        if config.train.optimizer.weight_decay_param.echo:\n",
    "            print(\n",
    "                '================================== SGD nest, momentum = {}, wd = {}'\n",
    "                .format(config.train.optimizer.momentum,\n",
    "                        config.train.optimizer.weight_decay_param.base_decay))\n",
    "    return optimizer\n",
    "def set_weight_decay(model, skip_list=(), skip_keywords=(), echo=False):\n",
    "    has_decay = []\n",
    "    no_decay = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue  # frozen weights\n",
    "        if 'identity.weight' in name:\n",
    "            has_decay.append(param)\n",
    "            if echo:\n",
    "                print(f\"{name} USE weight decay\")\n",
    "        elif len(param.shape) == 1 or name.endswith(\".bias\") or (\n",
    "                name in skip_list) or check_keywords_in_name(\n",
    "                    name, skip_keywords):\n",
    "            no_decay.append(param)\n",
    "            if echo:\n",
    "                print(f\"{name} has no weight decay\")\n",
    "        else:\n",
    "            has_decay.append(param)\n",
    "            if echo:\n",
    "                print(f\"{name} USE weight decay\")\n",
    "\n",
    "    return [{'params': has_decay}, {'params': no_decay, 'weight_decay': 0.}]\n",
    "def check_keywords_in_name(name, keywords=()):\n",
    "    isin = False\n",
    "    for keyword in keywords:\n",
    "        if keyword in name:\n",
    "            isin = True\n",
    "    return isin\n",
    "# build_optimizer(config, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c702fa59d8122797944f957dba7be4f979da7d1929b3cf04b128c443ec5b9bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
