{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34881024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# the layer number of each stage\n",
    "stage_layer_max = [2, 4, 14, 2]\n",
    "# the channel number of each layer\n",
    "layer = np.array([64, 128, 256, 512])\n",
    "# the descending step of channel \n",
    "step = 8 \n",
    "op = ['repvgg','vgg'] # every stage\n",
    "\n",
    "maxpool = ['T','F']\n",
    "ratio = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "def search_space(stage_layer, layer):\n",
    "    sum = 1\n",
    "    for i in stage_layer_max:\n",
    "        sum *= i\n",
    "    for j in layer:\n",
    "        sum *= len(ratio)\n",
    "    return sum*16*16\n",
    "print(search_space(stage_layer_max,layer)/1e8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choices \n",
    "choices(range(1,3))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices(ratio)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'T', 'F', 'F']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机生成模型\n",
    "\n",
    "stage_layer = [choices(range(1,stage_layer_max[0]+1))[0],choices(range(1,stage_layer_max[1]+1))[0],choices(range(1,stage_layer_max[2]+1))[0],choices(range(1,stage_layer_max[3]+1))[0]]\n",
    "# stage_layer\n",
    "stage_ratio = [choices(ratio)[0],choices(ratio)[0],choices(ratio)[0],choices(ratio)[0]]\n",
    "# stage_ratio\n",
    "with_maxpool = [choices(maxpool)[0],choices(maxpool)[0],choices(maxpool)[0],choices(maxpool)[0]]\n",
    "with_maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def save_dict_to_yaml(dict_value: dict, save_path: str):\n",
    "    \"\"\"dict保存为yaml\"\"\"\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(yaml.dump(dict_value, allow_unicode=True))\n",
    " \n",
    "def read_yaml_to_dict(yaml_path: str):\n",
    "    with open(yaml_path) as file:\n",
    "        dict_value = yaml.load(file.read(), Loader=yaml.FullLoader)\n",
    "        return dict_value\n",
    "config = {}\n",
    "dir = \n",
    "config['model'] = {'stage_layer':stage_layer,'stage_ratio': stage_ratio ,'with_maxpool':with_maxpool}\n",
    "config['train'] = {'start_epoch': 0, \"epochs\": 300, \"warmup_epochs\": 20, \"warmup_lr\": 1.0e-3, \n",
    "                   \"lr_scheduler\":{\"min_lr\":1.0e-5,\"decay_epochs\":30,\"decay_rate\":0.1},\"batch_size\":256,\"loss\":\"crossentropy\",\n",
    "                   \"optimizer\":{\"name\":\"SGD\",\"base_lr\":0.0005,\"momentum\":0.9,\"eps\":1.0e-8,\"betas\":[0.9,0.999],\"weight_decay_param\":{\"decay\":0.05,\"echo\":False}},\n",
    "                   \"label_smoothing\":0.1,\"clip_grad\":0.0,\"ema_alpha\":0.0,\"ema_update_period\":8,\"use_l2_norm\":True,\n",
    "                   \"no_weight_decay\": \"rbr_dense\"}\n",
    "config['val'] = {\"batch_size\":128} \n",
    "config['output'] = {\"print_freq\":100,\"epoch_print_freq\":1,\"save_freq\":20,\n",
    "                    \"dir\": }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练自动生成yaml\n",
    "# loss acc train val\n",
    "# 存test最好的\n",
    "# loss crossentropy\n",
    "# SGD\n",
    "# lr 0.1，0.001（repvgg\n",
    "# weight decay 0.005，0.05（repvgg\n",
    "# 直接量化repvgg， repvgg block 的量化，需要考虑lr和weight decay\n",
    "\n",
    "# 随机生成模型配置，训\n",
    "# 得到小数据集\n",
    "# acc predictor， 是配置 （nas + predictor\n",
    "# 搜索算法 RL EL （chip memory 约束项，要提acc，降latency\n",
    "# 硬件性能 \n",
    "# hardware performance \n",
    "# 硬件结构固定，公式计算硬件性能\n",
    "# few-shot learning\n",
    "# meta-learning\n",
    "# 搜索考虑memory\n",
    "\n",
    "# 1. search space\n",
    "# 2. 重点是predictor\n",
    "# 3. \n",
    "\n",
    "# random genea\n",
    "# 进一步， 搜索对不同硬件都高效的网络\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14167530720"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*4*14*2*31*63*127*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qhy/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
    "    result = nn.Sequential()\n",
    "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
    "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Naive RepVGG without SE module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.non_linearity = nn.ReLU()\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
    "        else:\n",
    "            self.rbr_identity = nn.BatchNorm2d(num_features=out_channels) if out_channels == in_channels and stride == 1 else None\n",
    "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return self.non_linearity(self.rbr_reparam(inputs))\n",
    "        \n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "        \n",
    "        return self.non_linearity(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out)\n",
    "    \n",
    "    def get_custom_L2(self):\n",
    "        K3 = self.rbr_dense.conv.weight\n",
    "        K1 = self.rbr_1x1.conv.weight\n",
    "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
    "\n",
    "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
    "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
    "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
    "        return l2_loss_eq_kernel + l2_loss_circle\n",
    "\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])\n",
    "    \n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, nn.Sequential):\n",
    "            kernel = branch.conv.weight\n",
    "            running_mean = branch.bn.running_mean\n",
    "            running_var = branch.bn.running_var\n",
    "            gamma = branch.bn.weight\n",
    "            beta = branch.bn.bias\n",
    "            eps = branch.bn.eps\n",
    "        else:\n",
    "            assert isinstance(branch, nn.BatchNorm2d)\n",
    "            if not hasattr(self, 'id_tensor'):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
    "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.running_mean\n",
    "            running_var = branch.running_var\n",
    "            gamma = branch.weight\n",
    "            beta = branch.bias\n",
    "            eps = branch.eps\n",
    "        std = (running_var + eps).sqrt()\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'rbr_reparam'):\n",
    "            return\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels, kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride, padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
    "        self.rbr_reparam.weight.data = kernel\n",
    "        self.rbr_reparam.bias.data = bias\n",
    "        self.__delattr__('rbr_dense')\n",
    "        self.__delattr__('rbr_1x1')\n",
    "        if hasattr(self, 'rbr_identity'):\n",
    "            self.__delattr__('rbr_identity')\n",
    "        if hasattr(self, 'id_tensor'):\n",
    "            self.__delattr__('id_tensor')\n",
    "        self.deploy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepVGGBlock(\n",
       "  (non_linearity): ReLU()\n",
       "  (rbr_dense): Sequential(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (rbr_1x1): Sequential(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RepVGGBlock(3,64,3,padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding == 1 是为了保证尺寸不变\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    '''\n",
    "    根据配置表，返回模型层列表\n",
    "    '''\n",
    "    layers = [] # 层列表初始化\n",
    "\n",
    "    in_channels = 3 # 输入3通道图像\n",
    "\n",
    "    # 遍历配置列表\n",
    "    for v in cfg:\n",
    "        if v == 'M': # 添加池化层\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else: # 添加卷积层\n",
    "\n",
    "            # 3×3 卷积\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "\n",
    "            # 卷积-->批归一化（可选）--> ReLU激活\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "\n",
    "            # 通道数方面，下一层输入即为本层输出\n",
    "            in_channels = v\n",
    "\n",
    "    # 以sequencial类型返回模型层列表\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# make_layers(cfgs['A'], batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG通用网络模型\n",
    "    输入features为网络的特征提取部分网络层列表\n",
    "    分类数为 1000\n",
    "    '''\n",
    "    def __init__(self, features, dataset = 'cifar10', init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        if dataset == 'cifar10':\n",
    "            num_classes = 10\n",
    "        elif dataset == 'cifar100':\n",
    "            num_classes = 100\n",
    "            \n",
    "        # 特征提取部分\n",
    "        self.features = features\n",
    "\n",
    "        # 自适应平均池化，特征图池化到 7×7 大小\n",
    "        # 为了让下一步的flatten得到一个固定长度的向量\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        # 分类部分\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),   # 512*7*7 --> 4096\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),          # 4096 --> 4096\n",
    "            nn.ReLU(True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),   # 4096 --> 1000\n",
    "        )\n",
    "\n",
    "        # 权重初始化\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 特征提取\n",
    "        x = self.features(x)\n",
    "        # 自适应平均池化\n",
    "        x = self.avgpool(x)\n",
    "        # 特征图展平成向量\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 分类器分类输出\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        '''\n",
    "        权重初始化\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # 卷积层使用 kaimming 初始化\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                # 偏置初始化为0\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # 批归一化层权重初始化为1 \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            # 全连接层权重初始化\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c702fa59d8122797944f957dba7be4f979da7d1929b3cf04b128c443ec5b9bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
